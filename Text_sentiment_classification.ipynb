{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is a model of Text sentiment classification.\\nLi Teng\\n10.11.2021\\n\\n\\nThe process will be done in following step:\\n\\n1.load config\\n\\n2.build model\\n\\n3.load data and clean it.\\n\\n4.training and recording \\n\\n5.validation\\n\\n6.hotflip\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This is a model of Text sentiment classification.\n",
    "Li Teng\n",
    "10.11.2021\n",
    "\n",
    "\n",
    "The process will be done in following step:\n",
    "\n",
    "1.load config\n",
    "\n",
    "2.build model\n",
    "\n",
    "3.load data and clean it.\n",
    "\n",
    "4.training and recording \n",
    "\n",
    "5.validation\n",
    "\n",
    "6.hotflip\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import DefaultConfig\n",
    "from Text_Cnn import ConvNet, dynamical_padding, get_vocab_list, class_to_tensor\n",
    "import os\n",
    "import heapq\n",
    "from utils import load_flattened_documents, load_datasets, get_label, get_meanloss_and_wi, load_documents\n",
    "from datasets_preprocessing import clean_datasets, Movie_Classif_Dataset, get_part_of_speech\n",
    "from datasets_preprocessing import doc_to_tag, get_word_tag_dict, get_wi_tag\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import nltk\n",
    "\n",
    "'''\n",
    "loading config \n",
    "'''\n",
    "Conf = DefaultConfig()\n",
    "\n",
    "VOCAB_SIZE = Conf.vocab_size\n",
    "BATCH_SIZE = Conf.batch_size\n",
    "Lr = Conf.lr\n",
    "EPOCHS = Conf.epochs\n",
    "DEVICE = Conf.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 2: build model\n",
    "'''\n",
    "Text_CNN = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 3: load data and preprocessing\n",
    "'''\n",
    "\n",
    "#load raw datasets\n",
    "data_root = os.path.join('Data', 'movies')\n",
    "documents = load_flattened_documents(data_root,None)\n",
    "documents = clean_datasets(documents)\n",
    "train, val, test = load_datasets(data_root)\n",
    "#load Train_Dataset\n",
    "Train_Dataset = Movie_Classif_Dataset(documents,train)\n",
    "#load into DataLoader\n",
    "Loader = DataLoader(dataset = Train_Dataset,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=dynamical_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 4: training and recording \n",
    "'''\n",
    "def train(epochs,model,device,dataloader,Lr):\n",
    "    '''\n",
    "    Training model\n",
    "    '''    \n",
    "    loss_func = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=Lr)\n",
    "    model.to(device)\n",
    "    #model.train()\n",
    "    for e in range(epochs):\n",
    "        loss_sum = 0\n",
    "        acc = 0\n",
    "        for i,(x,y) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_func(y_hat,y)\n",
    "            label_hat = torch.argmax(y_hat,dim=1)\n",
    "            acc += torch.sum(label_hat == y).item()\n",
    "            loss_sum += loss\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1)%100 == 0:\n",
    "                acc = acc/(BATCH_SIZE*100)\n",
    "                loss_average = loss_sum/100\n",
    "                print('epoch:{}, batch:{}, loss:{}, acc:{}'.format(e,i,loss_average.data,acc))\n",
    "                wandb.log({\"loss\": loss_average.data,\"acc\":acc})\n",
    "                # Optional\n",
    "                wandb.watch(model)\n",
    "                loss_sum = 0\n",
    "                acc = 0\n",
    "    torch.save(model,'Text_Cnn_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from Text_Cnn_1.pth\n"
     ]
    }
   ],
   "source": [
    "#load model if we have checkpoint\n",
    "if os.path.exists('Text_Cnn_1.pth'):\n",
    "    if torch.cuda.is_available():\n",
    "        Text_CNN = torch.load('Text_Cnn_1.pth').cuda()\n",
    "    else:\n",
    "        Text_CNN = torch.load('Text_Cnn_1.pth')\n",
    "    print('load from Text_Cnn_1.pth')\n",
    "else:\n",
    "    print('training model.')\n",
    "    wandb.init(project='Text_Cnn',entity='teng_li')\n",
    "    train(EPOCHS,Text_CNN,DEVICE,Loader,Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/teng_li/Text_Cnn3/runs/3ijjncpi\" target=\"_blank\">fresh-forest-1</a></strong> to <a href=\"https://wandb.ai/teng_li/Text_Cnn3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, batch:99, loss:0.7008103132247925, acc:0.515\n",
      "epoch:0, batch:199, loss:0.6993375420570374, acc:0.52\n",
      "epoch:0, batch:299, loss:0.7014205455780029, acc:0.465\n",
      "epoch:0, batch:399, loss:0.6985161304473877, acc:0.505\n",
      "epoch:0, batch:499, loss:0.6908286213874817, acc:0.515\n",
      "epoch:0, batch:599, loss:0.6981061697006226, acc:0.465\n",
      "epoch:0, batch:699, loss:0.6973797678947449, acc:0.48\n",
      "epoch:0, batch:799, loss:0.6960407495498657, acc:0.495\n",
      "epoch:1, batch:99, loss:0.6951991319656372, acc:0.495\n",
      "epoch:1, batch:199, loss:0.6830211877822876, acc:0.585\n",
      "epoch:1, batch:299, loss:0.6974428296089172, acc:0.525\n",
      "epoch:1, batch:399, loss:0.6965981125831604, acc:0.48\n",
      "epoch:1, batch:499, loss:0.6983054280281067, acc:0.43\n",
      "epoch:1, batch:599, loss:0.6970250010490417, acc:0.485\n",
      "epoch:1, batch:699, loss:0.6955912113189697, acc:0.51\n",
      "epoch:1, batch:799, loss:0.6932725310325623, acc:0.525\n",
      "epoch:2, batch:99, loss:0.6923266053199768, acc:0.535\n",
      "epoch:2, batch:199, loss:0.6897321343421936, acc:0.56\n",
      "epoch:2, batch:299, loss:0.6897286176681519, acc:0.545\n",
      "epoch:2, batch:399, loss:0.6877145171165466, acc:0.535\n",
      "epoch:2, batch:499, loss:0.6826672554016113, acc:0.55\n",
      "epoch:2, batch:599, loss:0.6858451962471008, acc:0.585\n",
      "epoch:2, batch:699, loss:0.6638370752334595, acc:0.605\n",
      "epoch:2, batch:799, loss:0.6594459414482117, acc:0.635\n",
      "epoch:3, batch:99, loss:0.5470932126045227, acc:0.72\n",
      "epoch:3, batch:199, loss:0.5517892241477966, acc:0.695\n",
      "epoch:3, batch:299, loss:0.4214736819267273, acc:0.79\n",
      "epoch:3, batch:399, loss:0.46457621455192566, acc:0.785\n",
      "epoch:3, batch:499, loss:0.46582287549972534, acc:0.775\n",
      "epoch:3, batch:599, loss:0.357138067483902, acc:0.835\n",
      "epoch:3, batch:699, loss:0.38338959217071533, acc:0.81\n",
      "epoch:3, batch:799, loss:0.4453682601451874, acc:0.79\n",
      "epoch:4, batch:99, loss:0.18272943794727325, acc:0.93\n",
      "epoch:4, batch:199, loss:0.09729624539613724, acc:0.96\n",
      "epoch:4, batch:299, loss:0.2289338856935501, acc:0.92\n",
      "epoch:4, batch:399, loss:0.20542337000370026, acc:0.91\n",
      "epoch:4, batch:499, loss:0.3366333246231079, acc:0.88\n",
      "epoch:4, batch:599, loss:0.22020816802978516, acc:0.905\n",
      "epoch:4, batch:699, loss:0.1659575253725052, acc:0.95\n",
      "epoch:4, batch:799, loss:0.24677570164203644, acc:0.905\n",
      "epoch:5, batch:99, loss:0.022270891815423965, acc:1.0\n",
      "epoch:5, batch:199, loss:0.017283521592617035, acc:1.0\n",
      "epoch:5, batch:299, loss:0.12634938955307007, acc:0.945\n",
      "epoch:5, batch:399, loss:0.10905736684799194, acc:0.97\n",
      "epoch:5, batch:499, loss:0.1144026443362236, acc:0.955\n",
      "epoch:5, batch:599, loss:0.11882227659225464, acc:0.975\n",
      "epoch:5, batch:699, loss:0.14272361993789673, acc:0.945\n",
      "epoch:5, batch:799, loss:0.12338191270828247, acc:0.945\n",
      "epoch:6, batch:99, loss:0.017330264672636986, acc:1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-adced6fbb5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#wandb.init(project='Text_Cnn',entity='teng_li')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Text_Cnn3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'teng_li'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mText_CNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9b55c70536d2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, device, dataloader, Lr)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# backward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# try a new one\n",
    "#Text_CNN = ConvNet()\n",
    "print('training model.')\n",
    "#wandb.init(project='Text_Cnn',entity='teng_li')\n",
    "wandb.init(project='Text_Cnn3',entity='teng_li')\n",
    "train(20,Text_CNN,DEVICE,Loader,Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 175\n",
      "total: 200\n",
      "acc =  0.875\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "step 5: validation\n",
    "'''\n",
    "#load Val_Dataset\n",
    "Val_Dataset = Movie_Classif_Dataset(documents,val)\n",
    "#load into DataLoader\n",
    "Val_Loader = DataLoader(dataset = Val_Dataset,\n",
    "                    batch_size = 1,\n",
    "                    shuffle=False)\n",
    "\n",
    "def validation(model,val_dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_dataloader:\n",
    "            y_hat = model.predict(x)\n",
    "            if y[0] == y_hat:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print('correct:',correct)\n",
    "    print('total:',total)\n",
    "    print('acc = ',correct/total)\n",
    "    \n",
    "validation(Text_CNN,Val_Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstep 6: hotflip\\nIn this step we will try to:\\n6.0 get word_level documents and sentence_level documents, and some config\\n6.1 get gradient of one word (w0_gradient)\\n6.2 N words which has max loss changed to flip (min (w0_gradient * w0_embed ))\\n6.3 the best word(wi) for each filped word (max (w0_gradient * wi_embed))\\n6.4 find best w0 and wi\\n6.5 replace w0 with wi \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "step 6: hotflip\n",
    "In this step we will try to:\n",
    "6.0 get word_level documents and sentence_level documents, and some config\n",
    "6.1 get gradient of one word (w0_gradient)\n",
    "6.2 N words which has max loss changed to flip (min (w0_gradient * w0_embed ))\n",
    "6.3 the best word(wi) for each filped word (max (w0_gradient * wi_embed))\n",
    "6.4 find best w0 and wi\n",
    "6.5 replace w0 with wi \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 6.0:get word_level documents and sentence_level documents, the tag_dict, and some config\n",
    "'''\n",
    "data_root = os.path.join('Data', 'movies')\n",
    "word_level_docs = clean_datasets(load_flattened_documents(data_root,None))\n",
    "sent_level_docs = load_documents(data_root)\n",
    "\n",
    "# loss function\n",
    "loss_func = nn.NLLLoss()\n",
    "# first of all we need to load Vocab\n",
    "Vocab_list = get_vocab_list()\n",
    "\n",
    "#some conf of hotflip\n",
    "vocab_size = Conf.search_size    # get from Conf.search_size\n",
    "beam_size = Conf.beam_search_size # get from Conf.beam_search_size\n",
    "change_word_num = Conf.change_word_num # get from Conf.change_word_num\n",
    "\n",
    "# get word tag dict from given sent_level_documents\n",
    "word_tag_dict = get_word_tag_dict(sent_level_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset those if we want to flip again, cause those data will be changed after flip\n",
    "# first time flip dont need to do this\n",
    "Val_Loader = DataLoader(dataset = Val_Dataset,\n",
    "                    batch_size = 1,\n",
    "                    shuffle=False)\n",
    "word_level_docs = clean_datasets(load_flattened_documents(data_root,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before flip: tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_800.txt\n",
      "flip time: 1\n",
      "{'w0': 'knockoffs', 'w0_place': 38, 'wi_id': 1543, 'wi': 'drugs', 'loss_estimate': tensor(0.0009, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0040, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'plot', 'w0_place': 366, 'wi_id': 877, 'wi': 'land', 'loss_estimate': tensor(0.0150, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'worse', 'w0_place': 529, 'wi_id': 39, 'wi': 'more', 'loss_estimate': tensor(0.0465, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'attempts', 'w0_place': 474, 'wi_id': 926, 'wi': 'calls', 'loss_estimate': tensor(0.0656, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0923, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'attempt', 'w0_place': 512, 'wi_id': 627, 'wi': 'try', 'loss_estimate': tensor(0.2115, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.1330, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_801.txt\n",
      "flip time: 1\n",
      "{'w0': 'wasting', 'w0_place': 246, 'wi_id': 911, 'wi': 'giving', 'loss_estimate': tensor(0.3534, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(2.9969, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'cheap', 'w0_place': 224, 'wi_id': 127, 'wi': 'good', 'loss_estimate': tensor(6.3014, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(5.5353, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'thriller', 'w0_place': 32, 'wi_id': 598, 'wi': 'action', 'loss_estimate': tensor(3.4404, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(5.9380, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'predictable', 'w0_place': 225, 'wi_id': 1845, 'wi': 'normal', 'loss_estimate': tensor(3.1246, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(6.5594, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'weary', 'w0_place': 21, 'wi_id': 286, 'wi': 'little', 'loss_estimate': tensor(2.6824, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(7.0520, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(4.0769e-05, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_802.txt\n",
      "flip time: 1\n",
      "{'w0': 'mess', 'w0_place': 366, 'wi_id': 493, 'wi': 'thing', 'loss_estimate': tensor(2.4220e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(5.3166e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'unfunny', 'w0_place': 285, 'wi_id': 1980, 'wi': 'character', 'loss_estimate': tensor(5.6674e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(6.7828e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'dull', 'w0_place': 369, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(9.1862e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'poorly', 'w0_place': 280, 'wi_id': 1853, 'wi': 'clearly', 'loss_estimate': tensor(0.0020, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'haphazardly', 'w0_place': 282, 'wi_id': 966, 'wi': 'quickly', 'loss_estimate': tensor(0.0764, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_803.txt\n",
      "flip time: 1\n",
      "{'w0': 'hopelessly', 'w0_place': 498, 'wi_id': 151, 'wi': 'still', 'loss_estimate': tensor(0.0022, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0019, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'uninteresting', 'w0_place': 418, 'wi_id': 1980, 'wi': 'character', 'loss_estimate': tensor(0.0047, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'plodding', 'w0_place': 35, 'wi_id': 1005, 'wi': 'moving', 'loss_estimate': tensor(0.0093, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0063, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'storyline', 'w0_place': 374, 'wi_id': 692, 'wi': 'film', 'loss_estimate': tensor(0.0205, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'sapping', 'w0_place': 648, 'wi_id': 1545, 'wi': 'rising', 'loss_estimate': tensor(0.1574, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.1763, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_804.txt\n",
      "flip time: 1\n",
      "{'w0': 'bad', 'w0_place': 343, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(0.1288, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'spends', 'w0_place': 63, 'wi_id': 539, 'wi': 'needs', 'loss_estimate': tensor(0.5210, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.3355, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'downright', 'w0_place': 201, 'wi_id': 738, 'wi': 'bit', 'loss_estimate': tensor(2.5606, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(2.3103, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'writer', 'w0_place': 320, 'wi_id': 1269, 'wi': 'friend', 'loss_estimate': tensor(5.1860, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(3.0663, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'accuses', 'w0_place': 102, 'wi_id': 1278, 'wi': 'believes', 'loss_estimate': tensor(3.0098, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(3.5100, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_805.txt\n",
      "flip time: 1\n",
      "{'w0': 'absurd', 'w0_place': 130, 'wi_id': 1225, 'wi': 'true', 'loss_estimate': tensor(0.0662, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.1958, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'overrated', 'w0_place': 75, 'wi_id': 746, 'wi': 'love', 'loss_estimate': tensor(1.6259, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.4837, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'pointless', 'w0_place': 163, 'wi_id': 1933, 'wi': 'sort', 'loss_estimate': tensor(4.2878, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(1.8766, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'chore', 'w0_place': 614, 'wi_id': 155, 'wi': 'even', 'loss_estimate': tensor(3.6967, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(2.2631, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'problems', 'w0_place': 133, 'wi_id': 992, 'wi': 'questions', 'loss_estimate': tensor(3.7388, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(3.1961, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(2.4368, grad_fn=<NllLossBackward>)\n",
      "class before flip: POS\n",
      "docid: negR_806.txt\n",
      "flip time: 1\n",
      "{'w0': 'mess', 'w0_place': 263, 'wi_id': 493, 'wi': 'thing', 'loss_estimate': tensor(23.3747, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(8.5638, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'weak', 'w0_place': 350, 'wi_id': 1057, 'wi': 'positive', 'loss_estimate': tensor(9.0322, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(10.3051, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'unable', 'w0_place': 341, 'wi_id': 848, 'wi': 'ready', 'loss_estimate': tensor(0.4910, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(10.3704, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'utterly', 'w0_place': 26, 'wi_id': 1082, 'wi': 'quite', 'loss_estimate': tensor(0.2843, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(10.4234, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w0': 'in', 'w0_place': 174, 'wi_id': 897, 'wi': 'throughout', 'loss_estimate': tensor(0.2400, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(10.6522, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(3.9696e-05, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_807.txt\n",
      "flip time: 1\n",
      "{'w0': 'clumsy', 'w0_place': 864, 'wi_id': 286, 'wi': 'little', 'loss_estimate': tensor(6.5800e-06, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'bad', 'w0_place': 1414, 'wi_id': 235, 'wi': 'lot', 'loss_estimate': tensor(8.4178e-06, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(4.2199e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'mess', 'w0_place': 46, 'wi_id': 493, 'wi': 'thing', 'loss_estimate': tensor(8.1044e-06, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(4.7802e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'bad', 'w0_place': 797, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(1.2983e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(4.9590e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'bad', 'w0_place': 609, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(2.2505e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(5.3404e-05, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(4.5217, grad_fn=<NllLossBackward>)\n",
      "class before flip: POS\n",
      "docid: negR_808.txt\n",
      "flip time: 1\n",
      "{'w0': 'silliness', 'w0_place': 238, 'wi_id': 1260, 'wi': 'fun', 'loss_estimate': tensor(5.5717, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(6.4559, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'crime', 'w0_place': 43, 'wi_id': 1304, 'wi': 'scene', 'loss_estimate': tensor(2.1260, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(6.6563, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'somehow', 'w0_place': 282, 'wi_id': 574, 'wi': 'feel', 'loss_estimate': tensor(1.3886, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(7.2613, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'tough', 'w0_place': 272, 'wi_id': 396, 'wi': 'important', 'loss_estimate': tensor(1.1746, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(7.9459, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'as', 'w0_place': 150, 'wi_id': 88, 'wi': 'if', 'loss_estimate': tensor(0.6547, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(8.0760, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(2.8252e-05, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_809.txt\n",
      "flip time: 1\n",
      "{'w0': 'pathetic', 'w0_place': 31, 'wi_id': 127, 'wi': 'good', 'loss_estimate': tensor(1.2200e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(3.7193e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'fiasco', 'w0_place': 7, 'wi_id': 1539, 'wi': 'debate', 'loss_estimate': tensor(1.6941e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(4.5895e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'bad', 'w0_place': 181, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(3.0015e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(8.0225e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'implausible', 'w0_place': 47, 'wi_id': 1225, 'wi': 'true', 'loss_estimate': tensor(5.9678e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(9.8820e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'seems', 'w0_place': 171, 'wi_id': 539, 'wi': 'needs', 'loss_estimate': tensor(6.8650e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_810.txt\n",
      "flip time: 1\n",
      "{'w0': 'cliche', 'w0_place': 156, 'wi_id': 626, 'wi': 'kind', 'loss_estimate': tensor(0.0003, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'embarrassed', 'w0_place': 469, 'wi_id': 1049, 'wi': 'happy', 'loss_estimate': tensor(0.0003, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'plot', 'w0_place': 527, 'wi_id': 1121, 'wi': 'movie', 'loss_estimate': tensor(0.0003, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'worse', 'w0_place': 420, 'wi_id': 1462, 'wi': 'greater', 'loss_estimate': tensor(0.0003, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'eve', 'w0_place': 183, 'wi_id': 113, 'wi': 'day', 'loss_estimate': tensor(0.0003, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_811.txt\n",
      "flip time: 1\n",
      "{'w0': 'bad', 'w0_place': 737, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(0.0007, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0012, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'pathetic', 'w0_place': 598, 'wi_id': 127, 'wi': 'good', 'loss_estimate': tensor(0.0042, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0059, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'idiotically', 'w0_place': 526, 'wi_id': 905, 'wi': 'actually', 'loss_estimate': tensor(0.0314, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'overdone', 'w0_place': 435, 'wi_id': 738, 'wi': 'bit', 'loss_estimate': tensor(0.3055, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0875, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'endless', 'w0_place': 174, 'wi_id': 1004, 'wi': 'huge', 'loss_estimate': tensor(0.2973, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.1087, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_812.txt\n",
      "flip time: 1\n",
      "{'w0': 'boring', 'w0_place': 59, 'wi_id': 125, 'wi': 'going', 'loss_estimate': tensor(1.7125e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(6.1272e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'laughable', 'w0_place': 191, 'wi_id': 1225, 'wi': 'true', 'loss_estimate': tensor(4.5534e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'uneventful', 'w0_place': 197, 'wi_id': 282, 'wi': 'early', 'loss_estimate': tensor(4.6902e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(7.1523e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'bad', 'w0_place': 361, 'wi_id': 235, 'wi': 'lot', 'loss_estimate': tensor(2.8150e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(7.3907e-05, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'horribly', 'w0_place': 66, 'wi_id': 138, 'wi': 'very', 'loss_estimate': tensor(2.7740e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_813.txt\n",
      "flip time: 1\n",
      "{'w0': 'plot', 'w0_place': 501, 'wi_id': 877, 'wi': 'land', 'loss_estimate': tensor(0.0002, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'worst', 'w0_place': 105, 'wi_id': 201, 'wi': 'best', 'loss_estimate': tensor(0.0008, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0020, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'bad', 'w0_place': 119, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(0.0130, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w0': 'bad', 'w0_place': 564, 'wi_id': 267, 'wi': 'great', 'loss_estimate': tensor(0.0632, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'dull', 'w0_place': 155, 'wi_id': 1535, 'wi': 'nice', 'loss_estimate': tensor(0.1625, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_814.txt\n",
      "flip time: 1\n",
      "{'w0': 'badly', 'w0_place': 183, 'wi_id': 112, 'wi': 'well', 'loss_estimate': tensor(0.0902, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.3543, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'failed', 'w0_place': 279, 'wi_id': 583, 'wi': 'wanted', 'loss_estimate': tensor(3.1411, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(1.6754, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'plot', 'w0_place': 173, 'wi_id': 1693, 'wi': 'murder', 'loss_estimate': tensor(3.9053, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(4.8500, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'are', 'w0_place': 125, 'wi_id': 1500, 'wi': 'seem', 'loss_estimate': tensor(2.5745, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(4.9123, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'banished', 'w0_place': 87, 'wi_id': 1095, 'wi': 'returned', 'loss_estimate': tensor(2.8002, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(5.4664, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(9.5840e-05, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_815.txt\n",
      "flip time: 1\n",
      "{'w0': 'fail', 'w0_place': 404, 'wi_id': 1713, 'wi': 'seek', 'loss_estimate': tensor(7.7355e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'lackluster', 'w0_place': 202, 'wi_id': 1057, 'wi': 'positive', 'loss_estimate': tensor(0.0002, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'incident', 'w0_place': 967, 'wi_id': 1304, 'wi': 'scene', 'loss_estimate': tensor(0.0001, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'so', 'w0_place': 938, 'wi_id': 1771, 'wi': 'perhaps', 'loss_estimate': tensor(0.0001, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'failures', 'w0_place': 208, 'wi_id': 1844, 'wi': 'actions', 'loss_estimate': tensor(0.0001, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_816.txt\n",
      "flip time: 1\n",
      "{'w0': 'bad', 'w0_place': 66, 'wi_id': 267, 'wi': 'great', 'loss_estimate': tensor(0.0023, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'bikini', 'w0_place': 137, 'wi_id': 1408, 'wi': 'girl', 'loss_estimate': tensor(0.0390, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n",
      "{'w0': 'stillborn', 'w0_place': 539, 'wi_id': 1601, 'wi': 'born', 'loss_estimate': tensor(0.1480, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.2975, grad_fn=<NllLossBackward>)\n",
      "flip time: 4\n",
      "{'w0': 'this', 'w0_place': 222, 'wi_id': 101, 'wi': 'any', 'loss_estimate': tensor(1.7365, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(1.8468, grad_fn=<NllLossBackward>)\n",
      "flip time: 5\n",
      "{'w0': 'terrible', 'w0_place': 55, 'wi_id': 177, 'wi': 'know', 'loss_estimate': tensor(6.1675, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: POS\n",
      "loss after flip: tensor(3.5141, grad_fn=<NllLossBackward>)\n",
      "loss before flip: tensor(3.3140e-05, grad_fn=<NllLossBackward>)\n",
      "class before flip: NEG\n",
      "docid: negR_817.txt\n",
      "flip time: 1\n",
      "{'w0': 'sloppy', 'w0_place': 196, 'wi_id': 127, 'wi': 'good', 'loss_estimate': tensor(3.7724e-05, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "flip time: 2\n",
      "{'w0': 'mess', 'w0_place': 198, 'wi_id': 493, 'wi': 'thing', 'loss_estimate': tensor(0.0004, grad_fn=<DotBackward>)}\n",
      "predict after hotflip: NEG\n",
      "loss after flip: tensor(0.0031, grad_fn=<NllLossBackward>)\n",
      "flip time: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "hotflip: here I only handle the Val_docs as a sample\n",
    "'''\n",
    "\n",
    "#for each doc in val_dataloader\n",
    "for i,(x,y) in enumerate(Val_Loader):\n",
    "    y_hat = Text_CNN.single_instance_forward(x)\n",
    "    y_tensor = class_to_tensor(y)\n",
    "    # record the prediction and loss before hotflip\n",
    "    loss = loss_func(y_hat,y_tensor)\n",
    "    print('loss before flip:',loss)\n",
    "    print('class before flip:',Text_CNN.predict(x))\n",
    "    y = get_label(y)\n",
    "    docid = val[i].annotation_id\n",
    "    print('docid:',docid)\n",
    "    # get word_level and sent_level doc\n",
    "    word_doc = word_level_docs[docid]\n",
    "    sent_doc = sent_level_docs[docid]\n",
    "    # get tag for each word in doc\n",
    "    words_tag = doc_to_tag(sent_doc)\n",
    "    for n in range(change_word_num):\n",
    "        print('flip time:',n+1)\n",
    "        # step 6.1: get gradient\n",
    "        gradient = Text_CNN.get_gradient(x,y)\n",
    "        # step 6.2: get best w0\n",
    "        w0_list = []# a set to record mean loss of each w0\n",
    "        for w0_place,w0 in enumerate(word_doc):\n",
    "            w0_id = x[0][0][w0_place]\n",
    "            #print(w0)\n",
    "            w0_embed = Text_CNN.get_word_embedding(w0_id)\n",
    "            #print(w0_embed.shape)\n",
    "            w0_gradient = gradient[0][0][0][w0_place]\n",
    "            #print(w0_gradient.shape)\n",
    "            if w0_gradient.sum() != 0:\n",
    "                # now min (w0_gradient * w0_embed )\n",
    "                w0_loss_estimate = torch.dot(w0_gradient,w0_embed)\n",
    "                w0_list.append({'w0':w0,'w0_place':w0_place,'w0_gradient':w0_gradient,\n",
    "                                'w0_embed':w0_embed,'w0_loss_estimate':w0_loss_estimate})\n",
    "        # now let's get best w0 list\n",
    "        \n",
    "        best_w0 = heapq.nsmallest(beam_size,w0_list,key=lambda s: s['w0_loss_estimate'])\n",
    "        \n",
    "        #step 6.3: the best word(wi) for each filped word (max (w0_gradient * wi_embed))\n",
    "        final_list = []\n",
    "        for w0_info in best_w0:\n",
    "            w0 = w0_info['w0']\n",
    "            w0_place = w0_info['w0_place']\n",
    "            w0_tag = words_tag[w0_place] \n",
    "            w0_embed = w0_info['w0_embed']\n",
    "            best_wi = {'wi_id':None,'wi_embed':None,'wi_loss_estimate':0}\n",
    "            w0_gradient = w0_info['w0_gradient']\n",
    "            for wi_id in range(vocab_size):\n",
    "                # get word wi\n",
    "                wi_index = torch.tensor(wi_id)\n",
    "                wi = Vocab_list[wi_id]\n",
    "                \n",
    "                # POS constraint\n",
    "                # get the tag of wi\n",
    "                if wi in word_tag_dict:\n",
    "                    wi_tag = word_tag_dict[wi]\n",
    "                else:\n",
    "                    wi_tag = set()\n",
    "                    wi_tag.add(get_part_of_speech(wi))\n",
    "                #check w0_tag and wi_tag, if not same, skip to next word\n",
    "                if w0_tag not in wi_tag:\n",
    "                    continue\n",
    "                # check if the tag changed after flip. if changed, skip\n",
    "                tag = get_wi_tag(sent_doc,w0_place,wi)\n",
    "                if tag != w0_tag:\n",
    "                    continue\n",
    "                #end of the POS constraint    \n",
    "                \n",
    "                wi_embed = Text_CNN.get_word_embedding(wi_index)\n",
    "                # cos_sim constraint\n",
    "                if torch.cosine_similarity(w0_embed,wi_embed,dim=0)<0.3:\n",
    "                    continue\n",
    "                # end of cos_sim constraint\n",
    "                # up to now the wi is qualified, then we can record the best wi\n",
    "                wi_loss_estimate = torch.dot(w0_gradient,wi_embed)\n",
    "                if wi_loss_estimate > best_wi['wi_loss_estimate']:\n",
    "                    best_wi['wi_id'] = wi_id\n",
    "                    best_wi['wi_embed'] = wi_embed\n",
    "                    best_wi['wi_loss_estimate'] = wi_loss_estimate\n",
    "            #now we get best wi for w0\n",
    "            \n",
    "            if best_wi['wi_embed']==None:\n",
    "                continue\n",
    "            wi_embed = best_wi['wi_embed']\n",
    "            embed_diff = wi_embed - w0_embed\n",
    "            loss_estimate = torch.dot(w0_gradient,embed_diff)\n",
    "            final_list.append({'w0':w0_info['w0'],'w0_place':w0_info['w0_place'],'wi_id':best_wi['wi_id'],\n",
    "                               'wi':Vocab_list[best_wi['wi_id']],'loss_estimate':loss_estimate})\n",
    "            \n",
    "        # then step 6.4: get best w0 and wi\n",
    "        final_filp = heapq.nlargest(1,final_list,key=lambda s:s['loss_estimate'])[0]\n",
    "        print(final_filp)\n",
    "        w0_place = final_filp['w0_place']\n",
    "        wi_id = final_filp['wi_id']\n",
    "        wi = final_filp['wi']\n",
    "        # step 6.5: replace w0 with wi\n",
    "        x[0][0][w0_place] = wi_id\n",
    "        word_doc[w0_place] = wi\n",
    "        # predict the class of new doc\n",
    "        print('predict after hotflip:',Text_CNN.predict(x))\n",
    "        # comput loss after this flip\n",
    "        y_hat = Text_CNN.single_instance_forward(x)\n",
    "        loss = loss_func(y_hat,y_tensor)\n",
    "        print('loss after flip:',loss)\n",
    "        \n",
    "        \n",
    "   \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "tensor(0.0028, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(w0_embed.shape)\n",
    "print(wi_embed.shape)\n",
    "print(torch.cosine_similarity(w0_embed,wi_embed,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "finished\n",
    "'''  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
